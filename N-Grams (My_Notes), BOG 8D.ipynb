{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"to be or not to be\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens=text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrm = nltk.bigrams(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be, be or, or not, not to, to be\n"
     ]
    }
   ],
   "source": [
    "print(*map(' '.join, bigrm), sep=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'this is a foo bar sentences and i want to ngramize it'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sixgrams = ngrams(sentence.split(), n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is', 'a', 'foo', 'bar', 'sentences', 'and', 'i')\n",
      "('is', 'a', 'foo', 'bar', 'sentences', 'and', 'i', 'want')\n",
      "('a', 'foo', 'bar', 'sentences', 'and', 'i', 'want', 'to')\n",
      "('foo', 'bar', 'sentences', 'and', 'i', 'want', 'to', 'ngramize')\n",
      "('bar', 'sentences', 'and', 'i', 'want', 'to', 'ngramize', 'it')\n"
     ]
    }
   ],
   "source": [
    "for i in sixgrams:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'this is a foo bar sentences and i want to ngramize it'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n= n-1                # n is a no# of ngrams, but it should be less then the total lenght of sentence vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = ngrams(sentence.split(), n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is', 'a', 'foo', 'bar', 'sentences', 'and', 'i', 'want', 'to', 'ngramize')\n",
      "('is', 'a', 'foo', 'bar', 'sentences', 'and', 'i', 'want', 'to', 'ngramize', 'it')\n"
     ]
    }
   ],
   "source": [
    "for grams in n_grams:\n",
    "     print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_Grams(text,n): \n",
    "    # split sentences into tokens\n",
    "    #tokens=re.split(\"\\\\s+\",text)\n",
    "    tokens=text.split()\n",
    "    ngrams=[] \n",
    "    # collect the n-grams\n",
    "    for i in range(len(tokens)-n+1):\n",
    "     temp=[tokens[j] for j in range(i,i+n)]\n",
    "     ngrams.append(\" \".join(temp)) \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=\"the quick brown fox jumps the lazy dog\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumps', 'the', 'lazy', 'dog']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_Grams(text1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the quick',\n",
       " 'quick brown',\n",
       " 'brown fox',\n",
       " 'fox jumps',\n",
       " 'jumps the',\n",
       " 'the lazy',\n",
       " 'lazy dog']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_Grams(text1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the quick brown',\n",
       " 'quick brown fox',\n",
       " 'brown fox jumps',\n",
       " 'fox jumps the',\n",
       " 'jumps the lazy',\n",
       " 'the lazy dog']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_Grams(text1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the sentence: Bag of Words a technique of linguistics\n",
      "Enter the value of n: 5\n",
      "('Bag', 'of', 'Words', 'a', 'technique')\n",
      "('of', 'Words', 'a', 'technique', 'of')\n",
      "('Words', 'a', 'technique', 'of', 'linguistics')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "sentence = input(\"Enter the sentence: \")\n",
    "n = int(input(\"Enter the value of n: \"))\n",
    "n_grams = ngrams(sentence.split(), n)\n",
    "for grams in n_grams:\n",
    "    print(grams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''The first function we will implement is to extract the words from a document using regular expressions.\n",
    " As we do so, we will be converting all words to lower case and exclude our stop words.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def tokenize_sentences(sentences):\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        w = extract_words(sentence)\n",
    "        words.extend(w)\n",
    "        \n",
    "    words = sorted(list(set(words)))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Next, we implement our tokenize_sentences function. This function builds our vocabulary by looping through\n",
    " all our documents (sentences), extracting the words from each, removing duplicates using the set function and \n",
    " returning a sorted list of words.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_words(sentence):\n",
    "    ignore_words = ['a']\n",
    "    words = re.sub(\"[^w]\", \" \",  sentence).split() \n",
    "    words_cleaned = [w.lower() for w in words if w not in ignore_words]\n",
    "    return words_cleaned   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagofwords(sentence, words):\n",
    "    sentence_words = extract_words(sentence)\n",
    "    # frequency word count\n",
    "    bag = np.zeros(len(words))\n",
    "    for sw in sentence_words:\n",
    "        for i,word in enumerate(words):\n",
    "            if word == sw: \n",
    "                bag[i] += 1\n",
    "                \n",
    "    return np.array(bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Our last function is the implementation of the bag of words model. This function takes an input of a sentence \n",
    "and words (our vocabulary). It then extracts the words from the input sentence using the previously defined function. \n",
    "It creates a vector of zeros using numpy zeros function with a length of the number of words in our vocabulary.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"Machine learning is great\",\"Natural Language Processing is a complex field\",\n",
    "\"Natural Language Processing is used in machine learning\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = tokenize_sentences(sentences)\n",
    "bagofwords(\"Machine learning is great\", vocabulary)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 5000) \n",
    "\n",
    "train_data_features = vectorizer.fit_transform(sentences)\n",
    "\n",
    "vectorizer.transform([\"Machine learning is great\",\"Natural Language Processing is a complex field\",\n",
    "\"Natural Language Processing is used in machine learning\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "doc1 = 'It was the best of times'\n",
    "doc2 = 'It was the worst of times'\n",
    "doc3 = 'It was the age of wisdome'\n",
    "doc4 = 'It was the age of foolishness'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ammara\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>best</th>\n",
       "      <th>foolishness</th>\n",
       "      <th>it</th>\n",
       "      <th>of</th>\n",
       "      <th>the</th>\n",
       "      <th>times</th>\n",
       "      <th>was</th>\n",
       "      <th>wisdome</th>\n",
       "      <th>worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  best  foolishness  it  of  the  times  was  wisdome  worst\n",
       "0    0     1            0   1   1    1      1    1        0      0\n",
       "1    0     0            0   1   1    1      1    1        0      1\n",
       "2    1     0            0   1   1    1      0    1        1      0\n",
       "3    1     0            1   1   1    1      0    1        0      0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([doc1,doc2,doc3,doc4])\n",
    "df_bow_sklearn = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names())\n",
    "df_bow_sklearn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
